///////////////////////////////////////////////////////////////////////////////////////////////////
// QUICK REFERENCE GUIDE - å¿«é€Ÿå‚è€ƒæŒ‡å—
// ğŸ“ ç”¨é€”: DATæ€§èƒ½æµ‹è¯•å•å…ƒæµ‹è¯•å®ç° - User Story 1 & 2
// ğŸ”„ æµç¨‹: User Story â†’ Acceptance Criteria â†’ Test Cases â†’ Implementation
// ğŸ“‚ åˆ†ç±»: DataPerformance US-1&2 - High throughput and low latency verification
// ğŸ¯ é‡ç‚¹: ååé‡éªŒè¯ã€å»¶è¿Ÿæµ‹é‡ã€APIæ€§èƒ½åˆ†æ
///////////////////////////////////////////////////////////////////////////////////////////////////

///////////////////////////////////////////////////////////////////////////////////////////////////
//======>BEGIN OF OVERVIEW OF THIS UNIT TESTING FILE===============================================
/**
 * @brief
 *  DATæ€§èƒ½æµ‹è¯•å®ç° - éªŒè¯IOCæ¡†æ¶ä¸­DATæœåŠ¡çš„é«˜ååé‡å’Œä½å»¶è¿Ÿæ€§èƒ½ç‰¹æ€§
 *
 *-------------------------------------------------------------------------------------------------
 *++èƒŒæ™¯è¯´æ˜ï¼š
 *  æœ¬æµ‹è¯•æ–‡ä»¶å®ç°DAT(Data Transfer)æœåŠ¡çš„æ€§èƒ½éªŒè¯ï¼Œä¸“æ³¨äºUS-1å’ŒUS-2çš„æ ¸å¿ƒæ€§èƒ½éœ€æ±‚
 *  é‡ç‚¹å…³æ³¨ååé‡ã€å»¶è¿Ÿç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡çš„æµ‹é‡å’ŒéªŒè¯
 *  ç¡®ä¿ç³»ç»Ÿåœ¨å„ç§è´Ÿè½½æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ç¬¦åˆé¢„æœŸ
 *
 *  å…³é”®æ¦‚å¿µï¼š
 *  - High-Throughput Testing: é«˜ååé‡æµ‹è¯•ï¼ŒéªŒè¯å¤§è´Ÿè½½æ•°æ®ä¼ è¾“é€Ÿç‡
 *  - Low-Latency Measurement: ä½å»¶è¿Ÿæµ‹é‡ï¼ŒéªŒè¯APIå“åº”æ—¶é—´ç‰¹æ€§
 *  - Performance Benchmarking: æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿
 *  - Resource Monitoring: èµ„æºä½¿ç”¨ç›‘æ§ï¼Œç¡®ä¿æ•ˆç‡ä¼˜åŒ–
 *
 *  æµ‹è¯•èŒƒå›´ï¼š
 *  - US-1: é«˜ååé‡æ•°æ®ä¼ è¾“éªŒè¯ (1KBåˆ°256KBè´Ÿè½½)
 *  - US-2: ä½å»¶è¿ŸAPIè°ƒç”¨éªŒè¯ (å¾®ç§’çº§å“åº”æ—¶é—´)
 *  - æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ
 *  - æˆåŠŸç‡å’Œé”™è¯¯ç‡ç»Ÿè®¡
 */
//======>END OF OVERVIEW OF THIS UNIT TESTING FILE=================================================

///////////////////////////////////////////////////////////////////////////////////////////////////
//======>BEGIN OF UNIT TESTING DESIGN==============================================================

///////////////////////////////////////////////////////////////////////////////////////////////////
//======>BEGIN OF USER STORY=======================================================================
/**************************************************************************************************
 * @brief ã€User Storyã€‘
 *
 *  US-1: AS a high-throughput DAT application developer,
 *    I WANT to verify that IOC_sendDAT operations achieve optimal throughput,
 *   SO THAT I can ensure maximum data transfer rates under various payload sizes
 *      AND validate batch transfer efficiency for bulk data operations,
 *      AND implement high-performance data streaming solutions.
 *
 *  US-2: AS a low-latency DAT application developer,
 *    I WANT to verify that DAT operations maintain minimal end-to-end latency,
 *   SO THAT I can ensure real-time data delivery requirements are met
 *      AND validate API call response times are within acceptable limits,
 *      AND implement time-critical data communication systems.
 *
 *************************************************************************************************/
//======>END OF USER STORY=========================================================================

///////////////////////////////////////////////////////////////////////////////////////////////////
//=======>BEGIN OF ACCEPTANCE CRITERIA=============================================================
/**************************************************************************************************
 * @brief ã€Acceptance Criteriaã€‘
 *
 * [@US-1] High-throughput DAT operations verification
 *  AC-1: GIVEN a DAT link configured for bulk data transfer,
 *         WHEN sending large payloads (1KB to 256KB) repeatedly,
 *         THEN throughput should achieve target rates (e.g., >10MB/s for test environment)
 *              AND throughput should scale with payload size efficiently
 *              AND bulk transfer operations should maintain consistent performance.
 *
 *  AC-2: GIVEN multiple payload sizes for throughput testing,
 *         WHEN measuring transfer rates across different data volumes,
 *         THEN larger payloads should demonstrate improved efficiency
 *              AND performance metrics should be accurately collected
 *              AND success rate should remain above 95%.
 *
 *---------------------------------------------------------------------------------------------------
 * [@US-2] Low-latency DAT operations verification
 *  AC-1: GIVEN a DAT link optimized for minimal latency,
 *         WHEN sending small messages (256 bytes) with immediate delivery,
 *         THEN end-to-end latency should be within target limits (e.g., <10ms for test)
 *              AND latency should be consistent across message iterations
 *              AND jitter should be minimal for stable performance.
 *
 *  AC-2: GIVEN IOC_sendDAT API calls for latency measurement,
 *         WHEN measuring API call response times precisely,
 *         THEN API latency should be minimal and predictable
 *              AND P95/P99 latencies should be within acceptable bounds
 *              AND success rate should be very high (>99%).
 */
//=======>END OF ACCEPTANCE CRITERIA================================================================

///////////////////////////////////////////////////////////////////////////////////////////////////
//======>BEGIN OF TEST CASES=======================================================================
/**************************************************************************************************
 * @brief ã€Test Casesã€‘
 *
 * [@AC-1,US-1] High-throughput bulk data transfer
 *  TC-1:
 *      @[Name]: verifyBulkDataThroughput_byLargePayloads_expectOptimalRates
 *      @[Purpose]: éªŒè¯å¤§è´Ÿè½½æ•°æ®ä¼ è¾“çš„ååé‡æ€§èƒ½ï¼Œç¡®ä¿è¾¾åˆ°ç›®æ ‡ä¼ è¾“é€Ÿç‡
 *      @[Brief]: ä½¿ç”¨1KBåˆ°256KBä¸åŒè´Ÿè½½å¤§å°æµ‹è¯•ååé‡ï¼ŒéªŒè¯æ€§èƒ½éšè´Ÿè½½æ‰©å±•
 *      @[Throughput_Focus]: æµ‹è¯•æœ€å¤§æ•°æ®ä¼ è¾“é€Ÿç‡å’Œè´Ÿè½½å¤§å°å¯¹æ€§èƒ½çš„å½±å“
 *
 * [@AC-2,US-1] Throughput performance metrics validation
 *  TC-2:
 *      @[Name]: verifyThroughputMetrics_byPerformanceCollection_expectAccurateAnalysis
 *      @[Purpose]: éªŒè¯æ€§èƒ½æŒ‡æ ‡æ”¶é›†çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
 *      @[Brief]: æ”¶é›†å¹¶åˆ†æååé‡æµ‹è¯•è¿‡ç¨‹ä¸­çš„å„é¡¹æ€§èƒ½æ•°æ®
 *      @[Metrics_Focus]: æµ‹è¯•æ€§èƒ½æ•°æ®æ”¶é›†å’Œç»Ÿè®¡åˆ†æçš„æ­£ç¡®æ€§
 *
 *---------------------------------------------------------------------------------------------------
 * [@AC-1,US-2] Low-latency small message delivery
 *  TC-1:
 *      @[Name]: verifyAPIResponseTime_byCallLatency_expectMicrosecondLevel
 *      @[Purpose]: éªŒè¯APIè°ƒç”¨å“åº”æ—¶é—´ï¼Œç¡®ä¿ä½å»¶è¿Ÿè¦æ±‚æ»¡è¶³
 *      @[Brief]: æµ‹é‡IOC_sendDATçš„APIè°ƒç”¨å»¶è¿Ÿï¼ŒéªŒè¯å“åº”æ—¶é—´ç¨³å®šæ€§
 *      @[Latency_Focus]: æµ‹è¯•APIçº§åˆ«çš„å“åº”æ—¶é—´å’Œå»¶è¿Ÿä¸€è‡´æ€§
 *
 * [@AC-2,US-2] Latency statistical analysis
 *  TC-2:
 *      @[Name]: verifyLatencyDistribution_byStatisticalAnalysis_expectConsistentPerformance
 *      @[Purpose]: éªŒè¯å»¶è¿Ÿåˆ†å¸ƒç‰¹æ€§å’Œç»Ÿè®¡æŒ‡æ ‡
 *      @[Brief]: åˆ†æP95ã€P99å»¶è¿Ÿå’ŒæŠ–åŠ¨ï¼ŒéªŒè¯æ€§èƒ½ç¨³å®šæ€§
 *      @[Statistics_Focus]: æµ‹è¯•å»¶è¿Ÿç»Ÿè®¡åˆ†æå’Œæ€§èƒ½ä¸€è‡´æ€§éªŒè¯
 *
 *************************************************************************************************/
//======>END OF TEST CASES=========================================================================
//======>END OF UNIT TESTING DESIGN================================================================

///////////////////////////////////////////////////////////////////////////////////////////////////
//======BEGIN OF UNIT TESTING IMPLEMENTATION=======================================================
#include "UT_DataPerformance.h"

//===TEMPLATE OF UT CASE===
/**
 * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 * â•‘                           ğŸš€ BULK DATA THROUGHPUT VERIFICATION                          â•‘
 * â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
 * â•‘ @[Name]: verifyBulkDataThroughput_byLargePayloads_expectOptimalRates                    â•‘
 * â•‘ @[Steps]: ğŸ”§ setup performance test environment â†’ ğŸ¯ execute multi-size payload tests   â•‘
 * â•‘          â†’ âœ… verify throughput metrics â†’ ğŸ§¹ cleanup resources                          â•‘
 * â•‘ @[Expect]: Throughput â‰¥10MB/s, success rate >95%, consistent performance scaling       â•‘
 * â•‘ @[Notes]: Core performance test case validating maximum data transfer capabilities      â•‘
 * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */
TEST(UT_DataPerformance, verifyBulkDataThroughput_byLargePayloads_expectOptimalRates) {
    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                ğŸ”§ SETUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    printf("ğŸ§ª [TEST] verifyBulkDataThroughput_byLargePayloads_expectOptimalRates\n");

    // Initialize performance test configuration
    PerformanceTestConfig_T testConfig;
    testConfig.TestDurationSec = std::chrono::seconds(5);
    testConfig.TargetThroughputMBps = 10.0;
    testConfig.MaxAcceptableLatencyMs = 10.0;

    // Setup test service and connections
    IOC_SrvID_T testSrvID = IOC_ID_INVALID;
    IOC_LinkID_T senderLinkID = IOC_ID_INVALID;

    // Service setup
    IOC_SrvArgs_T srvArgs = {};
    IOC_Helper_initSrvArgs(&srvArgs);
    srvArgs.SrvURI.pProtocol = IOC_SRV_PROTO_FIFO;
    srvArgs.SrvURI.pHost = IOC_SRV_HOST_LOCAL_PROCESS;
    srvArgs.SrvURI.pPath = "test/performance/throughput";
    srvArgs.UsageCapabilites = IOC_LinkUsageDatSender;
    srvArgs.Flags = IOC_SRVFLAG_AUTO_ACCEPT;

    IOC_Result_T result = IOC_onlineService(&testSrvID, &srvArgs);
    ASSERT_EQ(IOC_RESULT_SUCCESS, result) << "Service setup failed";

    // Client connection setup
    IOC_ConnArgs_T connArgs = {};
    IOC_Helper_initConnArgs(&connArgs);
    connArgs.SrvURI = srvArgs.SrvURI;
    connArgs.Usage = IOC_LinkUsageDatReceiver;

    result = IOC_connectService(&senderLinkID, &connArgs, NULL);
    ASSERT_EQ(IOC_RESULT_SUCCESS, result) << "Client connection setup failed";

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ¯ BEHAVIOR PHASE                                       â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    printf("ğŸ¯ BEHAVIOR: Testing bulk data throughput with multiple payload sizes\n");

    PerformanceCollector collector;
    collector.StartCollection();

    // Test different payload sizes for throughput analysis
    std::vector<size_t> payloadSizes = {1024, 4096, 16384, 65536, 262144};  // 1KB to 256KB

    for (size_t payloadSize : payloadSizes) {
        printf("ğŸ“Š [PAYLOAD] Testing throughput with %zu bytes\n", payloadSize);

        std::vector<char> testData = CreatePerformanceTestData(payloadSize, false);
        auto startTime = std::chrono::high_resolution_clock::now();
        size_t iterations = 100;

        for (size_t i = 0; i < iterations; ++i) {
            IOC_DatDesc_T sendDesc = {};
            IOC_initDatDesc(&sendDesc);
            sendDesc.Payload.pData = testData.data();
            sendDesc.Payload.PtrDataSize = payloadSize;
            sendDesc.Payload.PtrDataLen = payloadSize;

            auto opStart = std::chrono::high_resolution_clock::now();
            IOC_Result_T sendResult = IOC_sendDAT(senderLinkID, &sendDesc, NULL);
            auto opEnd = std::chrono::high_resolution_clock::now();

            if (sendResult == IOC_RESULT_SUCCESS) {
                double latencyUs = std::chrono::duration<double, std::micro>(opEnd - opStart).count();
                collector.RecordLatency(latencyUs);
                collector.RecordOperation(payloadSize);
            } else {
                collector.RecordError();
                printf("âš ï¸ [WARNING] IOC_sendDAT failed with result %d\n", sendResult);
            }

            std::this_thread::sleep_for(std::chrono::microseconds(100));
        }

        auto endTime = std::chrono::high_resolution_clock::now();
        double durationSec = std::chrono::duration<double>(endTime - startTime).count();
        double mbps = (iterations * payloadSize) / (durationSec * 1024.0 * 1024.0);
        printf("ğŸ“ˆ [RESULT] Payload %zu bytes: %.2f MB/s\n", payloadSize, mbps);
    }

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                âœ… VERIFY PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    PerformanceMetrics_T metrics = collector.FinishCollection();
    PrintPerformanceReport(metrics, "Bulk Data Throughput Test");

    //@KeyVerifyPoint-1: Overall throughput should meet target
    VERIFY_THROUGHPUT_TARGET(metrics, testConfig.TargetThroughputMBps);

    //@KeyVerifyPoint-2: Success rate should be high
    VERIFY_SUCCESS_RATE_TARGET(metrics, 0.95);

    //@KeyVerifyPoint-3: Performance should be reasonable
    VERIFY_LATENCY_TARGET(metrics, testConfig.MaxAcceptableLatencyMs);

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ§¹ CLEANUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    if (senderLinkID != IOC_ID_INVALID) {
        IOC_closeLink(senderLinkID);
    }
    if (testSrvID != IOC_ID_INVALID) {
        IOC_offlineService(testSrvID);
    }
}

/**
 * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 * â•‘                          â±ï¸ API RESPONSE TIME VERIFICATION                               â•‘
 * â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
 * â•‘ @[Name]: verifyAPIResponseTime_byCallLatency_expectMicrosecondLevel                     â•‘
 * â•‘ @[Steps]: ğŸ”§ setup low-latency test environment â†’ ğŸ¯ execute many small API calls       â•‘
 * â•‘          â†’ âœ… analyze latency statistics â†’ ğŸ§¹ cleanup resources                         â•‘
 * â•‘ @[Expect]: Avg latency <10ms, P99 <20ms, success rate >99%                             â•‘
 * â•‘ @[Notes]: API-level performance test ensuring real-time application requirements       â•‘
 * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */
TEST(UT_DataPerformance, verifyAPIResponseTime_byCallLatency_expectMicrosecondLevel) {
    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                ğŸ”§ SETUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    printf("ğŸ§ª [TEST] verifyAPIResponseTime_byCallLatency_expectMicrosecondLevel\n");

    // Setup performance test configuration for latency testing
    PerformanceTestConfig_T testConfig;
    testConfig.MaxAcceptableLatencyMs = 10.0;

    // Setup test service and connections
    IOC_SrvID_T testSrvID = IOC_ID_INVALID;
    IOC_LinkID_T senderLinkID = IOC_ID_INVALID;

    IOC_SrvArgs_T srvArgs = {};
    IOC_Helper_initSrvArgs(&srvArgs);
    srvArgs.SrvURI.pProtocol = IOC_SRV_PROTO_FIFO;
    srvArgs.SrvURI.pHost = IOC_SRV_HOST_LOCAL_PROCESS;
    srvArgs.SrvURI.pPath = "test/performance/latency";
    srvArgs.UsageCapabilites = IOC_LinkUsageDatSender;
    srvArgs.Flags = IOC_SRVFLAG_AUTO_ACCEPT;

    IOC_Result_T result = IOC_onlineService(&testSrvID, &srvArgs);
    ASSERT_EQ(IOC_RESULT_SUCCESS, result) << "Service setup failed";

    IOC_ConnArgs_T connArgs = {};
    IOC_Helper_initConnArgs(&connArgs);
    connArgs.SrvURI = srvArgs.SrvURI;
    connArgs.Usage = IOC_LinkUsageDatReceiver;

    result = IOC_connectService(&senderLinkID, &connArgs, NULL);
    ASSERT_EQ(IOC_RESULT_SUCCESS, result) << "Client connection setup failed";

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ¯ BEHAVIOR PHASE                                       â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    printf("ğŸ¯ BEHAVIOR: Measuring API call latency for small messages\n");

    PerformanceCollector collector;
    collector.StartCollection();

    const size_t messageSize = 256;
    const size_t testIterations = 1000;
    std::vector<char> testData = CreatePerformanceTestData(messageSize, false);

    for (size_t i = 0; i < testIterations; ++i) {
        IOC_DatDesc_T sendDesc = {};
        IOC_initDatDesc(&sendDesc);
        sendDesc.Payload.pData = testData.data();
        sendDesc.Payload.PtrDataSize = messageSize;
        sendDesc.Payload.PtrDataLen = messageSize;

        double latencyUs = MeasureOperationLatency([&]() {
            IOC_Result_T sendResult = IOC_sendDAT(senderLinkID, &sendDesc, NULL);
            if (sendResult != IOC_RESULT_SUCCESS) {
                collector.RecordError();
            }
        });

        collector.RecordLatency(latencyUs);
        collector.RecordOperation(messageSize);

        if (i % 100 == 0) {
            std::this_thread::sleep_for(std::chrono::microseconds(10));
        }
    }

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                âœ… VERIFY PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    PerformanceMetrics_T metrics = collector.FinishCollection();

    printf("\nğŸ“Š [LATENCY ANALYSIS]\n");
    printf("â”œâ”€ Iterations: %zu\n", testIterations);
    printf("â”œâ”€ Avg Latency: %.2f Î¼s\n", metrics.AvgLatencyUs);
    printf("â”œâ”€ P95 Latency: %.2f Î¼s\n", metrics.P95LatencyUs);
    printf("â””â”€ P99 Latency: %.2f Î¼s\n", metrics.P99LatencyUs);

    //@KeyVerifyPoint-1: Average latency should be low
    VERIFY_LATENCY_TARGET(metrics, testConfig.MaxAcceptableLatencyMs);

    //@KeyVerifyPoint-2: Success rate should be very high
    VERIFY_SUCCESS_RATE_TARGET(metrics, 0.99);

    //@KeyVerifyPoint-3: Jitter should be minimal
    double jitterMs = metrics.JitterUs / 1000.0;
    EXPECT_LE(jitterMs, testConfig.MaxAcceptableLatencyMs) << "API latency jitter too high: " << jitterMs << " ms";

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ§¹ CLEANUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    if (senderLinkID != IOC_ID_INVALID) {
        IOC_closeLink(senderLinkID);
    }
    if (testSrvID != IOC_ID_INVALID) {
        IOC_offlineService(testSrvID);
    }
}

//---------------------------------------------------------------------------------------------------------------------
class UT_DataPerformanceFixture : public ::testing::Test {
   protected:
    static void SetUpTestSuite() {
        printf("ğŸ”§ UT_DataPerformanceFixture->SETUP: SetUpTestSuite\n");
        // Global performance test environment initialization
    }

    static void TearDownTestSuite() {
        printf("ğŸ§¹ UT_DataPerformanceFixture->CLEANUP: TearDownTestSuite\n");
        // Global performance test environment cleanup
    }

    void SetUp() override {
        printf("ğŸ”§ UT_DataPerformanceFixture->SETUP: SetUp\n");

        // Initialize performance tracking for fixture tests
        __ResetPerformanceTracking(&senderPrivData);
        __ResetPerformanceTracking(&receiverPrivData);

        // Set conservative test configuration
        testConfig.TestDurationSec = std::chrono::seconds(5);
        testConfig.TargetThroughputMBps = 10.0;
        testConfig.MaxAcceptableLatencyMs = 10.0;
    }

    void TearDown() override {
        printf("ğŸ§¹ UT_DataPerformanceFixture->CLEANUP: TearDown\n");

        // Clean up connections
        if (receiverLinkID != IOC_ID_INVALID) {
            IOC_closeLink(receiverLinkID);
            receiverLinkID = IOC_ID_INVALID;
        }
        if (senderLinkID != IOC_ID_INVALID) {
            IOC_closeLink(senderLinkID);
            senderLinkID = IOC_ID_INVALID;
        }
        if (testSrvID != IOC_ID_INVALID) {
            IOC_offlineService(testSrvID);
            testSrvID = IOC_ID_INVALID;
        }
    }

    // Helper method to setup performance test scenario
    void setupPerformanceTestScenario() {
        IOC_SrvArgs_T srvArgs = {};
        IOC_Helper_initSrvArgs(&srvArgs);
        srvArgs.SrvURI.pProtocol = IOC_SRV_PROTO_FIFO;
        srvArgs.SrvURI.pHost = IOC_SRV_HOST_LOCAL_PROCESS;
        srvArgs.SrvURI.pPath = "test/performance/fixture";
        srvArgs.UsageCapabilites = IOC_LinkUsageDatSender;
        srvArgs.Flags = IOC_SRVFLAG_AUTO_ACCEPT;

        IOC_Result_T result = IOC_onlineService(&testSrvID, &srvArgs);
        ASSERT_EQ(IOC_RESULT_SUCCESS, result) << "Service setup failed";

        IOC_ConnArgs_T connArgs = {};
        IOC_Helper_initConnArgs(&connArgs);
        connArgs.SrvURI = srvArgs.SrvURI;
        connArgs.Usage = IOC_LinkUsageDatReceiver;

        result = IOC_connectService(&receiverLinkID, &connArgs, NULL);
        ASSERT_EQ(IOC_RESULT_SUCCESS, result) << "Client connection setup failed";

        senderLinkID = receiverLinkID;  // Same link for bi-directional testing
        senderPrivData.ServiceOnline = true;
        senderPrivData.LinkConnected = true;
        receiverPrivData.LinkConnected = true;
    }

    // Test data members
    __DatPerformancePrivData_T senderPrivData;
    __DatPerformancePrivData_T receiverPrivData;
    PerformanceTestConfig_T testConfig;
    IOC_SrvID_T testSrvID = IOC_ID_INVALID;
    IOC_LinkID_T senderLinkID = IOC_ID_INVALID;
    IOC_LinkID_T receiverLinkID = IOC_ID_INVALID;
};

/**
 * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 * â•‘                        ğŸ“ˆ THROUGHPUT SCALING VERIFICATION                               â•‘
 * â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
 * â•‘ @[Name]: verifyThroughputScaling_byPayloadSize_expectLinearGrowth                      â•‘
 * â•‘ @[Steps]: ğŸ”§ setup fixture test environment â†’ ğŸ¯ test varying payload sizes             â•‘
 * â•‘          â†’ âœ… verify scaling relationships â†’ ğŸ§¹ fixture cleanup                         â•‘
 * â•‘ @[Expect]: Throughput increases with payload size, performance data collected          â•‘
 * â•‘ @[Notes]: Fixture-based test validating throughput scaling characteristics             â•‘
 * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */
TEST_F(UT_DataPerformanceFixture, verifyThroughputScaling_byPayloadSize_expectLinearGrowth) {
    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                ğŸ”§ SETUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    setupPerformanceTestScenario();

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ¯ BEHAVIOR PHASE                                       â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    printf("ğŸ¯ DataPerformanceFixture->BEHAVIOR: verifyThroughputScaling_byPayloadSize_expectLinearGrowth\n");

    // Test throughput scaling with increasing payload sizes
    std::vector<size_t> payloadSizes = {1024, 8192, 65536};
    std::vector<double> throughputResults;

    for (size_t payloadSize : payloadSizes) {
        PerformanceCollector collector;
        collector.StartCollection();

        std::vector<char> testData = CreatePerformanceTestData(payloadSize, false);

        for (int i = 0; i < 50; ++i) {
            IOC_DatDesc_T sendDesc = {};
            IOC_initDatDesc(&sendDesc);
            sendDesc.Payload.pData = testData.data();
            sendDesc.Payload.PtrDataSize = payloadSize;
            sendDesc.Payload.PtrDataLen = payloadSize;

            double latencyUs = MeasureOperationLatency([&]() { IOC_sendDAT(senderLinkID, &sendDesc, NULL); });

            collector.RecordLatency(latencyUs);
            collector.RecordOperation(payloadSize);
        }

        PerformanceMetrics_T metrics = collector.FinishCollection();
        double mbps = metrics.BytesPerSecond / (1024.0 * 1024.0);
        throughputResults.push_back(mbps);

        printf("ğŸ“Š Payload %zu bytes: %.2f MB/s\n", payloadSize, mbps);
    }

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                âœ… VERIFY PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    //@KeyVerifyPoint-1: Throughput should generally increase with payload size
    ASSERT_GE(throughputResults.size(), 3) << "Should have throughput results for all payload sizes";

    //@KeyVerifyPoint-2: Larger payloads should show improved efficiency
    EXPECT_GT(throughputResults[2], throughputResults[0])
        << "Largest payload should have better throughput than smallest";

    //@KeyVerifyPoint-3: Performance data should be collected
    ASSERT_GT(senderPrivData.SendOperationCount.load(), 0) << "Should have recorded operations";

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ§¹ CLEANUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    // Cleanup handled by TearDown()
}

/**
 * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 * â•‘                        ğŸ”„ LATENCY CONSISTENCY VERIFICATION                              â•‘
 * â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
 * â•‘ @[Name]: verifyLatencyConsistency_byRepeatedCalls_expectStablePerformance              â•‘
 * â•‘ @[Steps]: ğŸ”§ setup fixture environment â†’ ğŸ¯ execute repeated operations                 â•‘
 * â•‘          â†’ âœ… verify latency consistency â†’ ğŸ§¹ fixture cleanup                           â•‘
 * â•‘ @[Expect]: Latency within bounds, low jitter, all operations recorded                  â•‘
 * â•‘ @[Notes]: Fixture-based test ensuring consistent latency performance                   â•‘
 * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */
TEST_F(UT_DataPerformanceFixture, verifyLatencyConsistency_byRepeatedCalls_expectStablePerformance) {
    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                ğŸ”§ SETUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    setupPerformanceTestScenario();

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ¯ BEHAVIOR PHASE                                       â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    printf("ğŸ¯ DataPerformanceFixture->BEHAVIOR: verifyLatencyConsistency_byRepeatedCalls_expectStablePerformance\n");

    PerformanceCollector collector;
    collector.StartCollection();

    const size_t messageSize = 512;
    const size_t iterations = 200;
    std::vector<char> testData = CreatePerformanceTestData(messageSize, false);

    for (size_t i = 0; i < iterations; ++i) {
        IOC_DatDesc_T sendDesc = {};
        IOC_initDatDesc(&sendDesc);
        sendDesc.Payload.pData = testData.data();
        sendDesc.Payload.PtrDataSize = messageSize;
        sendDesc.Payload.PtrDataLen = messageSize;

        double latencyUs = MeasureOperationLatency([&]() { IOC_sendDAT(senderLinkID, &sendDesc, NULL); });

        collector.RecordLatency(latencyUs);
        collector.RecordOperation(messageSize);

        // Track in private data
        senderPrivData.SendOperationCount++;
        senderPrivData.TotalBytesSent += messageSize;
    }

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                                âœ… VERIFY PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    PerformanceMetrics_T metrics = collector.FinishCollection();

    //@KeyVerifyPoint-1: Latency should be within acceptable bounds
    VERIFY_LATENCY_TARGET(metrics, testConfig.MaxAcceptableLatencyMs);

    //@KeyVerifyPoint-2: Jitter should be reasonable (low variability)
    double jitterMs = metrics.JitterUs / 1000.0;
    EXPECT_LE(jitterMs, testConfig.MaxAcceptableLatencyMs * 0.5)
        << "Latency jitter should be less than half the target latency";

    //@KeyVerifyPoint-3: All operations should be recorded
    ASSERT_EQ(senderPrivData.SendOperationCount.load(), iterations) << "Should have recorded all send operations";

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚                               ğŸ§¹ CLEANUP PHASE                                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    // Cleanup handled by TearDown()
}

//======END OF UNIT TESTING IMPLEMENTATION=========================================================
///////////////////////////////////////////////////////////////////////////////////////////////////
